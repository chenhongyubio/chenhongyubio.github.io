---
layout: post # 使用的布局（不需要改）
title:  Python爬虫  # 标题
subtitle: Python爬虫解决生活问题 #副标题
date: 2020-10-19 # 时间
author: CHY # 作者
header-img: img/wallhaven-vg7lv3.jpg #这篇文章标题背景图片
catalog: true # 是否归档
tags: #标签
  - 计算机
---
紧跟着《生信修炼手册》的步伐，开始学习了解Python爬虫开发，为后续打下基础。<br>

网络爬虫流程：
1. 网页内容下载
   1. urllib、request、selenium
2. html内容清洗
   1. 正则表达式、xpath表达式、beautifulsoup
3. 数据库内容存储
   1. sqlite、mysql、monogodb
![爬虫进阶路径](https://github.com/chenhongyubio/chenhongyubio.github.io/raw/master/img/爬虫进阶路径.png)<br>

#### 用户代理在爬虫中的应用
用户代理(User-Agent)，其具体内容为一行字符串，用来表征操作系统，浏览器版本等信息。<br>
通过修改http请求中的user-agent信息，可以将普通的爬虫程序伪装成一个浏览器的请求，从而绕过服务器反爬虫机制中对user-agent的限制。<br>
```
# urllib模块中设定UA
headers = {
     'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'
}
```

#### IP代理在网络爬虫中的应用
通过用户代理可以将普通的爬虫程序伪装成浏览器，而IP代理的作用则是用于突破目标服务器对同一IP访问频率的限制。<br>
在python中，通过urllib和requests这两个模块都实现IP代理。<br>
```
## urllib  检测IP可行性
proxy="http://119.8.44.244:8080"
 proxy_support=urllib.request.ProxyHandler({'http':proxy})
opener = urllib.request.build_opener(proxy_support)
urllib.request.install_opener(opener)
r = urllib.request.urlopen('http://icanhazip.com')
r.read()
```
```
## requests  检测IP可行性
import requests
proxies = {'http':'http://119.8.44.244:8080'}
r=requests.get("http://icanhazip.com", proxies=proxies)
r.text
```
```
## 完整流程
import requests
import random
import threading

# 获取代理IP
def get_proxy():
    ip_list = [
        'http://197.231.196.44:42461',
        'http://190.124.164.78:8080',
        'http://87.117.169.23:48705',
    ]
     return random.choice(ip_list)

# 下载单个网页
def getHtml(url, proxy):
    retry_count = 5
    while retry_count > 0:
        try:
            html = requests.get(url, proxies= {'http':proxy})
            return html
        except Exception:
            retry_count -= 1
    return None

# 每个线程的处理逻辑
def download_html(ko, semaphore, proxy):
    semaphore.acquire()
    url = 'https://www.genome.jp/dbget-bin/www_bget?ko:{}'.format(ko)
    out = './{}.kgml'.format(ko)
    r = getHtml(url, proxy)
    if r:
        print('{} download success!'.format(ko))
        with open(out, 'w') as fp:
            fp.write(r.text)
    else:
      print('{} download failed!'.format(ko))
    semaphore.release()

if __name__ == '__main__':
    ko_list = ['K{:05d}'.format(i) for i in range(1, 201)]

    thread_list = []
    semaphore = threading.BoundedSemaphore(100)
    for cnt, ko in enumerate(ko_list):
        if cnt % 10 == 0:
            proxy = get_proxy()
        p = threading.Thread(target = download_html, args = (ko, semaphore, proxy ))
        p.start()
        thread_list.append(p)
        
    for thread in thread_list:
        thread.join()
```


#### cookie在网络爬虫中的应用
主要用于解决需要登录的网页的问题。ookie是一种存储在本地浏览器中的用户认证信息。一般使用F12查看。<br>
cookie是一个动态信息，是和服务器交互之后生成的，具有时效性，在有效期内，cookie可以保持用户的登录状态，避免重复登录。当我们手动重新登录时，可以看到cookie的信息发生了变化.<br>
```
# urllib模块中的用法
headers = {
...   'Cookie': 'sessionid=feli4ngf23njptxxb0qma5tl04x8wc43; csrftoken=O9YSm7TMaIb2ZdqEnENJY1GBXj3xUE26',
...   'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36',
... }
request = urllib.request.Request('http://www.test.com', headers = headers)
response = urllib.request.urlopen(request)
```
```
# requests模块中的用法
import requests
headers = {
...   'Cookie': 'sessionid=feli4ngf23njptxxb0qma5tl04x8wc43; csrftoken=O9YSm7TMaIb2ZdqEnENJY1GBXj3xUE26',
...   'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36',
... }
r = requests.get('http://www.test.com', headers=headers)
```
```
# 模拟登录
import urllib.request
import http.cookiejar
url = 'http://www.test.com'
user = 'user'
password = 'passwd'
# 账号，密码的验证
pwdmgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()
pwdmgr.add_password(None, url, user, password)
auth_handler = urllib.request.HTTPBasicAuthHandler(pwdmgr)
# cookie处理
cookies = http.cookiejar.CookieJar()
cookie_handler = urllib.request.HTTPCookieProcessor(cookies)
# 构建opener
opener = urllib.request.build_opener(auth_handler, cookie_handler)
# 安装为全局
urllib.request.install_opener(opener)
r = urllib.request.urlopen(url)
for item in cookies:
... print(item.name+'='+item.value)
```