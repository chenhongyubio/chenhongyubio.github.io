---
layout: post # 使用的布局（不需要改）
title: Machine learning # 标题
subtitle: 周志华《机器学习》西瓜书学习笔记 #副标题
date: 2020-07-23 # 时间
author: CHY # 作者
header-img: img/wallhaven-机器学习.png #这篇文章标题背景图片
catalog: true # 是否归档
tags: #标签
  - 计算机
---

周志华《机器学习》西瓜书学习笔记。持续更新

#### 绪论

**机器学习**：在计算机上从数据中产生“模型”的算法，即“学习算法”。<br>
**数据集**：指记录的集合。<br>
**记录**：关于一个事件或对象在某方面的表现或性质的事项。<br>
**属性(特征)**; **属性值**<br>
**属性空间/样本空间/输入空间**：属性张成的空间<br>
每个记录都可在属性空间中找到自己的坐标位置，对应一个坐标向量，称为“特征向量”。<br>
**学习 learning/训练 training**：从数据中学得模型的过程<br>
**样例**：拥有标记信息的示例<br>
**分类**：预测为离散值的学习任务<br>
**回归**：预测为连续值的学习任务<br>
**预测任务**：希望通过对训练集进行学习，建立一个从输入空间到输出空间的映射。对二分类任务，通常令 y={-1,+1}或{0,1};对多分类任务,|y|>2;对回归任务，y=R,R 为实数集。<br>
**测试**：学得模型后，使用其进行预测的过程。<br>
根据训练数据是否拥有标记信息，学习任务可分为两大类：“监督学习"和"无监督学习”，**分类和回归是前者的代表，而聚类是后者的代表。**<br>
**独立同分布**：通常假设样本空间中全体样本服从一个未知的分布，当获得的每个样本都是独立地从这个分布采样获得的。<br>

**归纳**：从特殊到一般的“泛化”过程，即从具体的事实归结出一般性规律。<br>
**演绎**：从基础原理推演出具体状况，如通过公理和推理规则推导出与之相洽的定理。<br>
**概念学习/概念形成**：从训练数据集中学的概念，即狭义的归纳学习。<br>
**归纳偏好**：机器学习算法在学习过程中对某种类型假设的偏好。<br>
**奥卡姆剃刀**：在归纳偏好中，若有多个建设与观察一致，选择最简单那个。<br>

#### 模型评估与选择

模型选择的理想解决方案就是对候选模型的泛化误差进行评估，然后选择泛华误差最小的那个模型。<br>
需使用一个“测试集”来测试学习器对新样本的判别能力，然后以测试集上的测试误差作为泛化误差的近似，测试样本同样是从样本真实分布中独立同分布采用取得的，测试集应该尽可能与训练集互斥。<br>

##### 评估方法

##### 留出法

"留出法"：直接将数据集 D 划分为两个互斥的集合。<br>
需要注意，训练/测试集的划分要尽可能保持数据分布的一致性。<br>
在使用留出法时，一般要采用若干次随机划分、重复执行实验评估后取平均值作为留出法的评估结果。<br>
但训练和测试数据集的比例问题一直没有合理的解决办法，常见做法是将大约 2/3~4/5 的样本用于训练，剩余样本用于测试。<br>

##### 交叉验证法

原理：将数据集 D 划分为 k 个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即从数据集 D 中进行分层采样；然后每次用 k-1 个子集的并集作为训练集，余下的那个子集作为测试集；总共获得 K 组训练/测试集，从而进行 k 次训练和测试，最终返回的是 k 个测试结果的均值。也叫作"K 折交叉验证"。<br>

##### 自助法

原理：自助法主要是基于自助采样法为基础，具体而言就是，给定包含 m 个样本的数据集 D，对其采样产生数据集 D'(每次随机从 D 中挑选一个样本将其拷贝到数据集 D'，然后再将该样本放回数据集 D，使得该样本下次采样仍可能被抽到，即类似于有放回抽样，执行 m 次，便可得到含有 m 个样本的数据集 D')。<br>
通过自助采样，初始数据集 D 中将有 36.8%的样本未出现在采样数据集 D'中。<br>
一般使用采样数据集 D'作为训练数据集。<br>
自助法在数据集较小、难以有效划分训练和测试集时很有用，但由于其能够改变原始数据的分布，会引入偏差，所以当初始数据集足够时，留出法和交叉验证法更为常用。<br>

##### 调参与最终模型

参数对模型性能结果会产生很大的影响，而参数的选值是在实数范围内，所以很难一个一个参数的去训练出模型，常用做法是对每个参数选定一个范围和变化步长。<br>

#### 性能度量
